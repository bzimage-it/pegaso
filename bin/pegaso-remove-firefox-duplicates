#!/bin/bash

# Script to remove duplicate files created by Firefox with suffixes (1), (2), etc.
# Author: Script generated to handle Firefox duplicates
# Usage: pegaso-remove-firefox-duplicates [directory]

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Show help if requested
if [[ "${1:-}" == "--help" || "${1:-}" == "-h" ]]; then
    cat << 'EOF'
Usage: pegaso-remove-firefox-duplicates [options] [directory]

Removes duplicate files created by Firefox when downloading files with existing names.
Firefox adds suffixes like (1), (2), (3) before the file extension.

The script:
  - Identifies groups of files with the same base name (e.g.: document.pdf, document(1).pdf, document(2).pdf)
  - Verifies that files have identical content (using SHA256 checksum)
  - Keeps the most recent file (based on modification timestamp)
  - Renames the most recent file without suffix (if necessary)
  - Deletes all duplicates

Parameters:
  directory         Directory to process (default: current directory)
  -h, --help        Show this help message
  --no-verify       Skip content verification (use name only)
  --skip-checksum   Alias for --no-verify

Examples:
  pegaso-remove-firefox-duplicates                     # Process current directory
  pegaso-remove-firefox-duplicates ~/Downloads         # Process Downloads folder
  pegaso-remove-firefox-duplicates --no-verify .       # Process without verifying content

Notes:
  - By default, only files with identical content are removed (safe verification)
  - With --no-verify, files are considered duplicates by name only, without verifying content
  - The most recent file is always preserved
  - Supports multiple extensions (e.g.: .tar.gz)
  - Does not process subdirectories recursively
EOF
    exit 0
fi

# Flag to skip content verification
SKIP_CONTENT_CHECK=false

# Argument parsing
TARGET_DIR="."
while [[ $# -gt 0 ]]; do
    case "$1" in
        --no-verify|--skip-checksum)
            SKIP_CONTENT_CHECK=true
            shift
            ;;
        -*)
            echo -e "${RED}Error: Unknown option '$1'${NC}" >&2
            echo "Use --help to see available options" >&2
            exit 1
            ;;
        *)
            TARGET_DIR="$1"
            shift
            ;;
    esac
done

# Verify that the directory exists
if [[ ! -d "$TARGET_DIR" ]]; then
    echo -e "${RED}Error: Directory '$TARGET_DIR' does not exist${NC}" >&2
    exit 1
fi

# Convert to absolute path
TARGET_DIR=$(realpath "$TARGET_DIR")

echo -e "${BLUE}=== Firefox Duplicates Removal ===${NC}"
echo -e "Directory: ${GREEN}$TARGET_DIR${NC}"
echo ""

# Function to calculate checksum
calculate_checksum() {
    local file="$1"
    sha256sum "$file" | awk '{print $1}'
}

# Function to extract the base name from a file with or without suffix
# E.g.: "document(1).pdf" -> "document.pdf"
#       "document(1).tar.gz" -> "document.tar.gz"
#       "document.pdf" -> "document.pdf"
get_base_name() {
    local filename="$1"
    # Remove pattern (N) before extension (handles multiple extensions too)
    echo "$filename" | sed -E 's/\([0-9]+\)(\..+)$/\1/'
}

# Function to check if a file has the duplicate pattern
has_duplicate_pattern() {
    local filename="$1"
    [[ "$filename" =~ \([0-9]+\)\..+$ ]]
}

# Associative array to group files by base name
declare -A file_groups

# Find all files in the directory
while IFS= read -r -d '' filepath; do
    filename=$(basename "$filepath")
    
    # Extract the base name (without suffix (N))
    base_name=$(get_base_name "$filename")
    
    # Add the file to the group
    if [[ -n "${file_groups[$base_name]:-}" ]]; then
        file_groups[$base_name]="${file_groups[$base_name]}|$filepath"
    else
        file_groups[$base_name]="$filepath"
    fi
done < <(find "$TARGET_DIR" -maxdepth 1 -type f -print0)

# Statistics counters
total_processed=0
total_removed=0
total_space_freed=0

# Process each file group
for base_name in "${!file_groups[@]}"; do
    IFS='|' read -ra files <<< "${file_groups[$base_name]}"
    
    # If there's only one file, skip
    if [[ ${#files[@]} -lt 2 ]]; then
        continue
    fi
    
    # Check if at least one has the duplicate pattern
    has_duplicates=false
    for file in "${files[@]}"; do
        filename=$(basename "$file")
        if has_duplicate_pattern "$filename"; then
            has_duplicates=true
            break
        fi
    done
    
    # If no file has the duplicate pattern, skip
    if [[ "$has_duplicates" = false ]]; then
        continue
    fi
    
    echo -e "${YELLOW}Group found: $base_name${NC}"
    
    # Array to store file info
    declare -A file_info
    declare -a valid_files
    
    # Collect information about each file
    for file in "${files[@]}"; do
        if [[ ! -f "$file" ]]; then
            continue
        fi
        
        filename=$(basename "$file")
        size=$(stat -c%s "$file")
        mtime=$(stat -c%Y "$file")
        
        file_info["$file,size"]=$size
        file_info["$file,mtime"]=$mtime
        file_info["$file,checksum"]=""
        
        valid_files+=("$file")
        
        echo -e "  - $filename ($(numfmt --to=iec-i --suffix=B $size), $(date -d @$mtime '+%Y-%m-%d %H:%M:%S'))"
    done
    
    # If there's only one valid file, skip
    if [[ ${#valid_files[@]} -lt 2 ]]; then
        unset file_info
        unset valid_files
        continue
    fi
    
    # Array for identical files
    declare -a identical_files
    
    if [[ "$SKIP_CONTENT_CHECK" = true ]]; then
        # Mode without content verification: consider all files as identical
        echo -e "  ${BLUE}ℹ --no-verify mode: files are considered duplicates without content verification${NC}"
        identical_files=("${valid_files[@]}")
    else
        # Verify that all files have the same content
        # First compare by size, then by checksum if necessary
        reference_file="${valid_files[0]}"
        reference_size="${file_info[$reference_file,size]}"
        
        # Calculate checksum of reference file
        file_info["$reference_file,checksum"]=$(calculate_checksum "$reference_file")
        reference_checksum="${file_info[$reference_file,checksum]}"
        
        identical_files=("$reference_file")
        
        # Compare other files
        all_identical=true
        for file in "${valid_files[@]:1}"; do
            current_size="${file_info[$file,size]}"
            
            # If size is different, always calculate checksum
            if [[ "$current_size" != "$reference_size" ]]; then
                file_info["$file,checksum"]=$(calculate_checksum "$file")
                current_checksum="${file_info[$file,checksum]}"
                
                if [[ "$current_checksum" != "$reference_checksum" ]]; then
                    echo -e "  ${RED}✗ $(basename "$file") has different content (checksum: ${current_checksum:0:16}...)${NC}"
                    all_identical=false
                    continue
                fi
            else
                # Same size, calculate checksum for safety
                file_info["$file,checksum"]=$(calculate_checksum "$file")
                current_checksum="${file_info[$file,checksum]}"
                
                if [[ "$current_checksum" != "$reference_checksum" ]]; then
                    echo -e "  ${RED}✗ $(basename "$file") has different content${NC}"
                    all_identical=false
                    continue
                fi
            fi
            
            identical_files+=("$file")
        done
        
        # If not all files are identical, warn and skip
        if [[ "$all_identical" = false ]]; then
            echo -e "  ${YELLOW}⚠ Not all files are identical, group skipped${NC}"
            echo ""
            unset file_info
            unset valid_files
            unset identical_files
            continue
        fi
    fi
    
    # Find the most recent file
    newest_file=""
    newest_mtime=0
    for file in "${identical_files[@]}"; do
        mtime="${file_info[$file,mtime]}"
        if [[ $mtime -gt $newest_mtime ]]; then
            newest_mtime=$mtime
            newest_file="$file"
        fi
    done
    
    echo -e "  ${GREEN}✓ All files are identical${NC}"
    echo -e "  ${BLUE}→ Most recent file: $(basename "$newest_file")${NC}"
    
    # Determine the target name (without suffix)
    target_dir=$(dirname "$newest_file")
    target_path="$target_dir/$base_name"
    
    # If the most recent file is not already the one without suffix, rename it
    newest_filename=$(basename "$newest_file")
    if has_duplicate_pattern "$newest_filename"; then
        # If a file with the target name already exists, it's one of the files to remove
        # (because we verified they are all identical)
        if [[ -f "$target_path" && "$target_path" != "$newest_file" ]]; then
            rm -f "$target_path"
            echo -e "  ${GREEN}✓ Removed: $(basename "$target_path")${NC}"
        fi
        
        mv "$newest_file" "$target_path"
        echo -e "  ${GREEN}✓ Renamed: $newest_filename → $base_name${NC}"
        newest_file="$target_path"
    fi
    
    # Remove all other duplicate files
    for file in "${identical_files[@]}"; do
        if [[ "$file" != "$newest_file" ]]; then
            # Verify that the file still exists (it might have been renamed)
            if [[ -f "$file" ]]; then
                size="${file_info[$file,size]}"
                rm -f "$file"
                echo -e "  ${GREEN}✓ Removed: $(basename "$file")${NC}"
                total_removed=$((total_removed + 1))
                total_space_freed=$((total_space_freed + size))
            fi
        fi
    done
    
    total_processed=$((total_processed + 1))
    echo ""
    
    unset file_info
    unset valid_files
    unset identical_files
done

# Print final statistics
echo -e "${BLUE}=== Summary ===${NC}"
echo -e "Groups processed: ${GREEN}$total_processed${NC}"
echo -e "Files removed: ${GREEN}$total_removed${NC}"
echo -e "Space freed: ${GREEN}$(numfmt --to=iec-i --suffix=B $total_space_freed)${NC}"

if [[ $total_processed -eq 0 ]]; then
    echo -e "${YELLOW}No duplicates found.${NC}"
fi

exit 0
